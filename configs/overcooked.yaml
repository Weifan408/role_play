defaults:
  framework: torch
  rollouts:
    num_rollout_workers: 30
    # batch_mode: "complete_episodes"
  model:
    fcnet_hiddens: [128, 128]
    fcnet_activation: "relu"
    conv_activation: "relu"
    post_fcnet_hiddens: [128, 64]
    post_fcnet_activation: "relu"
    lstm_cell_size: 256
    lstm_use_prev_action: True
    use_lstm: True
  custom_model:
    num_lstm_layers: 1
  evaluation:
    evaluation_num_workers: 2
    evaluation_interval: 1
    evaluation_parallel_to_training: False
  training:
    train_batch_size: 8000
    sgd_minibatch_size: 2000
overcooked:
  rollouts:
    num_envs_per_worker: 5
  model:
    fcnet_hiddens: [32, 64]
    fcnet_activation: "relu"
    conv_filters: [[32, 3, 1], [64, 3, 1], [32, 3, 1]]
    conv_activation: "relu"
    post_fcnet_hiddens: [64, 64]
    post_fcnet_activation: "relu"
    lstm_cell_size: 128
    lstm_use_prev_action: True
    use_lstm: True
  custom_model:
    num_lstm_layers: 1
    actor_cnn_key: "featurize_obs"
    critic_cnn_key: "featurize_shared_obs"
    mlp_encoder_hiddens: [32]
    pre_lstm_hidden_layer_dims: [128, 64]
    pre_lstm_encoder_out_dim: 64
    class_num: 3
  env_config:
    env_zoo: "overcooked"
    name: "asymmetric_advantages"
    num_agents: 2
    initial_reward_shaping_factor: 1.0
    reward_shaping_factor: 1.0
    reward_shaping_horizon: 0
    use_detailed_rew_shaping: False
    store_traj: False
    use_render: False
    random_index: True
    random_start_prob: 0.0
    max_cycles: 400
    preference_num:  9
    w0: "0,0,0,0,r[-5:-5:1],r[-3:3:3],0,0,0,r[0:0:1],0,r[5:5:1],r[0:0:1],0,0,0,0,0,0,r[5:5:1],0,0,r[0:0:1],0,r[0:30:2],r[1:1:1]"
    w1: "0,0,0,0,r[-5:0:2],r[-3:3:3],0,0,0,r[0:5:2],0,r[-5:5:3],r[-5:5:3],0,0,0,0,0,0,r[0:5:2],0,0,r[-5:5:3],0,r[-30:30:3],r[0:1:2]"
    trial_length: 10
counter_circuit:
  env_config:
    name: "counter_circuit"
    preference_num:  10
    w0: "0,0,0,0,0,r[5:5:1],0,r[5:5:1],0,r[5:5:1],0,0,r[5:5:1],0,r[10:10:1],r[5:5:1],r[10:10:1],-5,0,0,0,0,r[3:3:1],r[3:3:1],r[0:10:2],1"
    w1: "0,0,0,0,0,r[-5:5:3],0,r[-5:5:3],0,r[-5:5:3],0,0,r[-5:5:3],0,r[-10:10:3],r[-5:5:3],r[0:10:2],-5,0,0,0,0,r[-3:3:3],r[-3:3:3],r[-10:10:3],1"
    trial_length: 10
eval:
  asymmetric_advantages:
    env_zoo: "overcooked"
    name: "asymmetric_advantages"
    version: "new"
    eval: True
    num_agents: 2
    initial_reward_shaping_factor: 1.0
    reward_shaping_factor: 1.0
    reward_shaping_horizon: 0
    random_index: False
    use_detailed_rew_shaping: False
    store_traj: False
    use_render: False
    random_start_prob: 0.0
    max_cycles: 400
    preference_num: 9
    w0: "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1"
    w1: "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1"
    w0_theta: "-1,0,0,1,0,1,0,1,1"
    w1_theta: "0,0,0,0,0,1,1,1,1"
  counter_circuit:
    env_zoo: "overcooked"
    name: "counter_circuit"
    version: "new"
    eval: True
    num_agents: 2
    initial_reward_shaping_factor: 1.0
    reward_shaping_factor: 1.0
    reward_shaping_horizon: 0
    use_detailed_rew_shaping: False
    random_index: False
    store_traj: False
    use_render: True
    random_start_prob: 0.0
    max_cycles: 400
    preference_num: 10
    w0: "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1"
    w1: "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1"
    w0_theta: "0,0,0,1,1,0,1,1,1,1"
    w1_theta: "0,0,0,0,0,0,0,0,0,0"
debug:
  rollouts:
    num_rollout_workers: 2
  stop_config:
    training_iteration: 20